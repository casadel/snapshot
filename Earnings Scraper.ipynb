{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is to write a function snapshot() that will obtain a company's quarterly earnings report from its investor relations website and output the relative metrics with which we are concerned (a \"snapshot\" of the report). In many cases, these metrics will consist of a company's current quarter earnings per share (EPS), current quarter revenue, and estimates for what these metrics will be in the next quarter, known as the \"guidance\". For many companies, however, there are various other metrics that concern us in addition to these, or in some cases instead of. Furthermore, with all the companies that report their quarterly earnings on their IR websites, there exists very little uniformity in the way in which their reports are structured. Thus, we have our work cut out for us.\n",
    "\n",
    "To start, we will try to parse the release of Netflix (NFLX). We are primarily concerned with identifying GAAP EPS and revenue along with guidance for these metrics for next quarter. For NFLX, we are also concerned with identifying net streaming adds.\n",
    "\n",
    "First task is to obtain the reports from the websites. In practice, we will want to have to program running maybe one minute before the expected earnings report time so that it is refreshing the page every tenth of a second or so and can have the report text the second it is released by the website. Reports are usually released as PDFs, although for NVDA they report in a press release in HTML format so we may have to account for this possibility.\n",
    "\n",
    "Most companies structure their reports such that there it consists of dialogue talking about the metrics followed by a table of comprehensive metrics and numbers. Will probably want to pull separate the two so they are individually parsable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "    \n",
    "    -Write the get pdf functions that will refresh on the quarterly results IR page and download the file\n",
    "        - get_nflx\n",
    "        - get_amzn\n",
    "        - get_twtr\n",
    "        - get_tsla\n",
    "        - get_aapl\n",
    "    -Write the table parsers for each company that will get the information we want for each company from the table and get paragraphs containing keywords\n",
    "        - nflx_parser\n",
    "        - amzn_parser\n",
    "        - twtr_parser\n",
    "        - tsla_parser\n",
    "        - aapl_parser\n",
    "        \n",
    "    - Wrap up the notebook so that it is usable from command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "import math\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "\n",
    "import pdfminer\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from cStringIO import StringIO\n",
    "\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFTextExtractionNotAllowed\n",
    "from pdfminer.converter import PDFPageAggregator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Get the PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nflx(link_dict):\n",
    "    while True:\n",
    "        page = requests.get(link_dict['NFLX'])\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        q3_html = soup.find_all('div', {'class': 'accBody'})[0]\n",
    "        docs = q3_html.find_all('a')\n",
    "        dwnload = []\n",
    "        found = False\n",
    "        for doc in docs:\n",
    "            if doc.text == 'Q316 Letter to shareholders':\n",
    "                link = doc['href']\n",
    "                found = True\n",
    "                break\n",
    "        if found:\n",
    "            break\n",
    "        time.sleep(1)\n",
    "    link = 'https://ir.netflix.com/' + link\n",
    "    pdfile = requests.get(link)\n",
    "    with open('nflx.pdf', 'wb') as f:\n",
    "        f.write(pdfile.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_amzn(link_dict):\n",
    "    while True:\n",
    "        page = requests.get(link_dict['AMZN'])\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        q3_html = soup.find_all('div', {'class': 'a-section article-copy'})[0]\n",
    "        docs = q3_html.find_all('a')\n",
    "        dwnload = []\n",
    "        found = False\n",
    "        for doc in docs:\n",
    "            if doc.text == 'Q3 2016 Financial Results':\n",
    "                link = doc['href']\n",
    "                found = True\n",
    "                break\n",
    "        if found:\n",
    "            break\n",
    "        time.sleep(1)\n",
    "    pdfile = requests.get(link)\n",
    "    with open('amzn.pdf', 'wb') as f:\n",
    "        f.write(pdfile.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Code to parse the PDFs, extract tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_layout_by_page(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts LTPage objects from a pdf file.\n",
    "    \n",
    "    slightly modified from\n",
    "    https://euske.github.io/pdfminer/programming.html\n",
    "    \"\"\"\n",
    "    laparams = LAParams()\n",
    "\n",
    "    fp = open(pdf_path, 'rb')\n",
    "    parser = PDFParser(fp)\n",
    "    document = PDFDocument(parser)\n",
    "\n",
    "    if not document.is_extractable:\n",
    "        raise PDFTextExtractionNotAllowed\n",
    "\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "    layouts = []\n",
    "    for page in PDFPage.create_pages(document):\n",
    "        interpreter.process_page(page)\n",
    "        layouts.append(device.get_result())\n",
    "\n",
    "    return layouts\n",
    "\n",
    "TEXT_ELEMENTS = [\n",
    "    pdfminer.layout.LTTextBox,\n",
    "    pdfminer.layout.LTTextBoxHorizontal,\n",
    "    pdfminer.layout.LTTextLine,\n",
    "    pdfminer.layout.LTTextLineHorizontal\n",
    "]\n",
    "\n",
    "def flatten(lst):\n",
    "    \"\"\"Flattens a list of lists\"\"\"\n",
    "    return [subelem for elem in lst for subelem in elem]\n",
    "\n",
    "\n",
    "def extract_characters(element):\n",
    "    \"\"\"\n",
    "    Recursively extracts individual characters from \n",
    "    text elements. \n",
    "    \"\"\"\n",
    "    if isinstance(element, pdfminer.layout.LTChar):\n",
    "        return [element]\n",
    "\n",
    "    if any(isinstance(element, i) for i in TEXT_ELEMENTS):\n",
    "        return flatten([extract_characters(e) for e in element])\n",
    "\n",
    "    if isinstance(element, list):\n",
    "        return flatten([extract_characters(l) for l in element])\n",
    "\n",
    "    return []\n",
    "\n",
    "def does_it_intersect(x, (xmin, xmax)):\n",
    "    return (x <= xmax and x >= xmin)\n",
    "\n",
    "def convert_to_rows(characters):\n",
    "    x_limit = 10\n",
    "    y_limit = 5\n",
    "    paragraph_limit = 20\n",
    "\n",
    "    rows = []\n",
    "    row = []\n",
    "    cell = \"\"\n",
    "    prior_x = None\n",
    "    prior_y = None\n",
    "\n",
    "    y_s = [];\n",
    "    x_s = [];\n",
    "    for c in characters:\n",
    "        c_x, c_y = math.floor((c.bbox[0] + c.bbox[2]) / 2), math.floor((c.bbox[1] + c.bbox[3]) / 2)\n",
    "        if prior_x is not None and not (c_x - prior_x <= x_limit and abs(c_y - prior_y) <= y_limit):\n",
    "            if abs(c_y - prior_y) > y_limit:\n",
    "                row.append(cell)\n",
    "\n",
    "                # find the right row\n",
    "                for i in xrange(len(rows)):\n",
    "                    if abs(y_s[i] - prior_y) <= y_limit:\n",
    "                        for j in xrange(len(x_s[i])):\n",
    "                            if prior_x < x_s[i][j]:\n",
    "                                rows[i] = rows[i][:j] + row + rows[i][j:]\n",
    "                                x_s[i] = x_s[i][:j] + [prior_x] + x_s[i][j:]\n",
    "                                break\n",
    "                        else:\n",
    "                            rows[i] += row\n",
    "                            x_s[i].append(prior_x)\n",
    "                            break\n",
    "                        break\n",
    "                else:\n",
    "                    rows.append(row)\n",
    "                    y_s.append(prior_y)\n",
    "                    x_s.append([prior_x])\n",
    "\n",
    "                cell = \"\"\n",
    "                row = []\n",
    "            elif c_x - prior_x > x_limit:\n",
    "                row.append(cell)\n",
    "                cell = \"\"\n",
    "\n",
    "        cell += c.get_text()\n",
    "        prior_x = c_x\n",
    "        prior_y = c_y\n",
    "\n",
    "    # handle the last row\n",
    "    row.append(cell)\n",
    "    for i in xrange(len(rows)):\n",
    "        if abs(y_s[i] - prior_y) <= y_limit:\n",
    "            for j in xrange(len(x_s[i])):\n",
    "                if prior_x < x_s[i][j]:\n",
    "                    rows[i] = rows[i][:j] + row + rows[i][j:]\n",
    "                    x_s[i] = x_s[i][:j] + [prior_x] + x_s[i][j:]\n",
    "                    break\n",
    "            else:\n",
    "                rows[i] += row\n",
    "                x_s[i].append(prior_x)\n",
    "                break\n",
    "            break\n",
    "    else:\n",
    "        rows.append(row)\n",
    "        y_s.append(prior_y)\n",
    "        x_s.append([prior_x])\n",
    "        \n",
    "    # insert blank rows between particularly separated lines\n",
    "    for i in xrange(len(y_s) - 2, -1, -1):\n",
    "        if abs(y_s[i] - y_s[i+1]) > paragraph_limit:\n",
    "            rows = rows[:i+1] + [[]] + rows[i+1:]\n",
    "    \n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Page Parsers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_pages(url, parser):\n",
    "    \n",
    "    page_layouts = extract_layout_by_page(url)\n",
    "    #objects_on_page = set(type(o) for o in page_layouts[3])\n",
    "\n",
    "    pages = []\n",
    "    for i in xrange(len(page_layouts)):\n",
    "        current_page = page_layouts[i]\n",
    "\n",
    "        texts = []\n",
    "\n",
    "        # seperate text and rectangle elements\n",
    "        for e in current_page:\n",
    "            if isinstance(e, pdfminer.layout.LTTextBoxHorizontal):\n",
    "                texts.append(e)\n",
    "\n",
    "        # sort them into \n",
    "        characters = extract_characters(texts)\n",
    "        pages.append(convert_to_rows(characters))\n",
    "    parser(pages)\n",
    "    \n",
    "def nflx_parser(pages):\n",
    "    for page in pages:\n",
    "        if len(page) > 1 and len(page[1]) > 0 and \"Consolidated Statements of Operations \" == page[1][0]:\n",
    "            for row in page:\n",
    "                if len(row) > 0 and row[0] == \"Revenues\":\n",
    "                    print(\"Revenue: \" + row[2])\n",
    "                # we want the first Basic in the table\n",
    "                elif len(row) > 0 and row[0] == \"Basic\":\n",
    "                    print(\"Basic EPS: \" + row[2])\n",
    "                    break\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def amzn_parser(pages):\n",
    "    page = pages[0]\n",
    "    tail = pages[1:]\n",
    "    flattened = list(itertools.chain.from_iterable(page))\n",
    "    if any(\"Consolidated Statements of Operations\" in s for s in flattened):\n",
    "        index1 = flattened.index('Total net sales ') + 1\n",
    "        print(\"Revenue: \" + flattened[index1]) \n",
    "        index2 = flattened.index('Basic earnings per share ') + 2\n",
    "        print(\"Basic EPS: \" + flattened[index2])\n",
    "    else:\n",
    "        if tail != []:\n",
    "            amzn_parser(tail)\n",
    "        else:\n",
    "            print \"No Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revenue: 2,290,188\n",
      "Basic EPS: 0.12\n"
     ]
    }
   ],
   "source": [
    "tix = ['TWTR', 'TSLA', 'NFLX', 'AMZN']\n",
    "links = ['https://investor.twitterinc.com/index.cfm', 'http://ir.tesla.com/', 'https://ir.netflix.com/results.cfm' ,'http://phx.corporate-ir.net/phoenix.zhtml?c=97664&p=irol-reportsOther']\n",
    "\n",
    "ir_dict = dict(zip(tix, links))\n",
    "        \n",
    "get_nflx(ir_dict)\n",
    "parse_pages('nflx.pdf', nflx_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
